# Testing Strategy

In Ledgerline I use a testing strategy that focuses on protecting the boundaries of the data flow, rather than exhaustively testing every transformation step in the middle. That means the strongest tests live where data enters the system and where data is consumed. At the ingestion edge, I validate grain, uniqueness, nullability, accepted values, and basic relational integrity so that upstream drift shows up immediately. If the source feed duplicates usage rows, if a catalog file drops a plan code, or if a price book arrives incomplete, those issues surface right away. This follows Zagni’s principle that early, precise failures are more valuable than sprawling test suites scattered throughout intermediate layers.

In the history models, I validate that each natural key produces a stable surrogate key, that version hashes behave consistently, and that no unexpected duplicates appear at the grain of (usage_hkey, report_date). Because Ledgerline’s history is append-only, the goal is simply to ensure changes are detected correctly and that incremental loads behave predictably. In the refined layer, I check that surrogate keys and natural keys stay aligned, that default keys don’t leak in unexpectedly, and that dimensions remain referentially clean.

At the consumption side, I tighten validation because these models behave as data contracts for the outside world. Mart models enforce schema contracts and grain contracts (unique combinations of the dimension keys). Any incompatible change fails the build early. The overall strategy is strong, defensive tests at the edges; structural and rule-based checks in the middle; and contract-level guarantees at the end. It scales across projects because it respects the natural flow of data and puts testing effort exactly where it catches the most meaningful failures.
